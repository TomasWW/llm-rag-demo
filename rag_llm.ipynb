{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyQigbVLy9fK7wBga9cY9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomasWW/llm-rag-demo/blob/main/rag_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X5pwPOh5S1V"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -U -q langchain langchain-openai langchain-community qdrant-client pypdf openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Load your OpenAI API key from Colab secrets\n",
        "OPENAI_API_KEY = userdata.get('apikey')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "OPENROUTER_API_BASE = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"mistralai/mistral-7b-instruct\",\n",
        "                 temperature=0.7,\n",
        "                 openai_api_base=OPENROUTER_API_BASE\n",
        "                 )\n",
        "\n",
        "# Example query using only LLM (without RAG)\n",
        "response = llm.invoke([\n",
        "    HumanMessage(content=\"Who won the Mundial de Clubes FIFA 2025\")\n",
        "])\n",
        "print(\"LLM-only response:\\n\", response.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "jpS-V2jx8yG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Qdrant setup\n",
        "\n",
        "from langchain.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Connect to Qdrant vector database\n",
        "DB_URL = userdata.get('dburl')\n",
        "RAG_ENGINE_API_KEY = userdata.get('apikeyrag-engine')\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=DB_URL,\n",
        "    api_key=RAG_ENGINE_API_KEY\n",
        ")\n",
        "\n",
        "COLLECTION_NAME = \"mundial_clubes_2025\"\n",
        "\n",
        "# Delete the collection if it already exists\n",
        "try:\n",
        "    qdrant_client.delete_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"Collection '{COLLECTION_NAME}' deleted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Collection did not exist: {e}\")\n"
      ],
      "metadata": {
        "id": "N0-Vt9ji-AwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and split PDF files\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load PDF files\n",
        "pdf_files = [\n",
        "    \"Anexo_Final de la Copa Mundial de Clubes de la FIFA 2025.pdf\",\n",
        "    \"Copa_Mundial_de_Clubes_de_la_FIFA_2025.pdf\"\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for file in pdf_files:\n",
        "    loader = PyPDFLoader(file)\n",
        "    docs += loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} pages from the PDF files.\")\n",
        "\n",
        "# Split text into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Created {len(chunks)} text chunks.\")\n"
      ],
      "metadata": {
        "id": "gR75AAmmsi80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Create embeddings using a HuggingFace model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Insert chunks into Qdrant\n",
        "qdrant = Qdrant.from_documents(\n",
        "    chunks,\n",
        "    embeddings,\n",
        "    location=DB_URL,\n",
        "    api_key=RAG_ENGINE_API_KEY,\n",
        "    collection_name=COLLECTION_NAME\n",
        ")\n",
        "\n",
        "print(f\"Upload complete. {len(chunks)} embeddings inserted into Qdrant.\")\n"
      ],
      "metadata": {
        "id": "Y87yz78yuRiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve the most relevant chunks for a user question\n",
        "def get_context(user_question, k=5):\n",
        "    \"\"\"\n",
        "    Retrieve the k most relevant text chunks from Qdrant\n",
        "    and concatenate them as context for the LLM.\n",
        "    \"\"\"\n",
        "    docs = qdrant.similarity_search(user_question, k=k)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "-ZNfseF79JyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User question\n",
        "user_input = \"Quien fue el campeon del Mundial de Clubes FIFA 2025\"\n",
        "\n",
        "# Retrieve context from vector DB\n",
        "context = get_context(user_input)\n",
        "print(\"Retrieved context:\\n\", context, \"\\n\\n\\n\")\n",
        "\n",
        "# Ask LLM using retrieved context\n",
        "response = llm.invoke([\n",
        "    HumanMessage(content=f\"Using the following context:\\n{context}\\n\\nAnswer the question: {user_input}\")\n",
        "])\n",
        "print(\"RAG-enhanced response:\\n\", response.content)"
      ],
      "metadata": {
        "id": "xbo-12Mx9OwG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}